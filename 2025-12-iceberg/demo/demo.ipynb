{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df80eaa",
   "metadata": {},
   "source": [
    "# PyIceberg Demo: Clickstream Analytics\n",
    "\n",
    "This notebook demonstrates Iceberg internals (Metadata, Snapshots, Manifests) using a local filesystem catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a31762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import TimestampType, LongType, StringType, NestedField\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import DayTransform\n",
    "\n",
    "# Clean up previous run\n",
    "if os.path.exists(\"tmp/warehouse\"):\n",
    "    shutil.rmtree(\"tmp/warehouse\")\n",
    "if os.path.exists(\"tmp/catalog.db\"):\n",
    "    os.remove(\"tmp/catalog.db\")\n",
    "\n",
    "os.makedirs(\"tmp/warehouse\", exist_ok=True)\n",
    "\n",
    "# Initialize Catalog\n",
    "catalog = load_catalog(\"local\", **{\n",
    "    \"type\": \"sql\",\n",
    "    \"uri\": \"sqlite:///tmp/catalog.db\",\n",
    "    \"warehouse\": \"tmp/warehouse\",\n",
    "})\n",
    "\n",
    "print(\"Catalog initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bd9b1",
   "metadata": {},
   "source": [
    "## 1. Create Table with Partitioning\n",
    "\n",
    "We define a schema for clickstream events and partition by **Day** of `event_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c47f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table default.events created\n"
     ]
    }
   ],
   "source": [
    "schema = Schema(\n",
    "    NestedField(1, \"event_time\", TimestampType(), required=False),\n",
    "    NestedField(2, \"user_id\", LongType(), required=False),\n",
    "    NestedField(3, \"event_name\", StringType(), required=False),\n",
    "    NestedField(4, \"event_properties\", StringType(), required=False),\n",
    ")\n",
    "\n",
    "partition_spec = PartitionSpec(\n",
    "    PartitionField(source_id=1, field_id=1000, transform=DayTransform(), name=\"event_time_day\")\n",
    ")\n",
    "\n",
    "table_name = \"default.events\"\n",
    "try:\n",
    "    catalog.drop_table(table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    catalog.drop_namespace(\"default\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "catalog.create_namespace(\"default\")\n",
    "\n",
    "table = catalog.create_table(\n",
    "    table_name,\n",
    "    schema=schema,\n",
    "    partition_spec=partition_spec,\n",
    ")\n",
    "\n",
    "print(f\"Table {table_name} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24806482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table structure:\n",
      "\u001b[01;34mtmp/warehouse/default/events/\u001b[0m\n",
      "└── \u001b[01;34mmetadata\u001b[0m\n",
      "    └── 00000-18a99734-edf1-4161-b86e-4b532224fba0.metadata.json\n",
      "\n",
      "2 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "# Inspect metadata\n",
    "print(\"\\nTable structure:\")\n",
    "!tree tmp/warehouse/default/events/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a9892a",
   "metadata": {},
   "source": [
    "## 2. Day 1: First Ingestion\n",
    "\n",
    "We generate data for `2025-12-01` and append it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "500193de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1 data appended\n"
     ]
    }
   ],
   "source": [
    "df_day1 = pd.DataFrame({\n",
    "    'event_time': [datetime(2025, 12, 1, 10, 0, 0), datetime(2025, 12, 1, 11, 30, 0)],\n",
    "    'user_id': [1, 2],\n",
    "    'event_name': ['login', 'view_item'],\n",
    "    'event_properties': ['{\"device\": \"mobile\"}', '{\"item_id\": 123}']\n",
    "})\n",
    "df_day1['event_time'] = df_day1['event_time'].astype('datetime64[us]')\n",
    "\n",
    "table.append(pa.Table.from_pandas(df_day1))\n",
    "print(\"Day 1 data appended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660a5eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table structure:\n",
      "\u001b[01;34mtmp/warehouse/default/events/\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "│   └── \u001b[01;34mevent_time_day=2025-12-01\u001b[0m\n",
      "│       └── 00000-0-cad83280-0b8f-4a8d-90d7-a7c39ca60430.parquet\n",
      "└── \u001b[01;34mmetadata\u001b[0m\n",
      "    ├── 00000-18a99734-edf1-4161-b86e-4b532224fba0.metadata.json\n",
      "    ├── 00001-bf263d9e-8932-4bbc-98bc-f853e8483c9d.metadata.json\n",
      "    ├── cad83280-0b8f-4a8d-90d7-a7c39ca60430-m0.avro\n",
      "    └── snap-7847261690880971748-0-cad83280-0b8f-4a8d-90d7-a7c39ca60430.avro\n",
      "\n",
      "4 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "# Inspect data and metadata\n",
    "print(\"\\nTable structure:\")\n",
    "!tree tmp/warehouse/default/events/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62910cb4",
   "metadata": {},
   "source": [
    "## 3. Day 2: New Partition\n",
    "\n",
    "We generate data for `2025-12-02`. This should create a new partition folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acb3dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 2 data appended\n"
     ]
    }
   ],
   "source": [
    "df_day2 = pd.DataFrame({\n",
    "    'event_time': [datetime(2025, 12, 2, 9, 15, 0), datetime(2025, 12, 2, 14, 20, 0)],\n",
    "    'user_id': [1, 3],\n",
    "    'event_name': ['login', 'checkout'],\n",
    "    'event_properties': ['{\"device\": \"web\"}', '{\"amount\": 99.99}']\n",
    "})\n",
    "df_day2['event_time'] = df_day2['event_time'].astype('datetime64[us]')\n",
    "\n",
    "table.append(pa.Table.from_pandas(df_day2))\n",
    "print(\"Day 2 data appended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8766310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table structure:\n",
      "\u001b[01;34mtmp/warehouse/default/events/\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "│   ├── \u001b[01;34mevent_time_day=2025-12-01\u001b[0m\n",
      "│   │   └── 00000-0-cad83280-0b8f-4a8d-90d7-a7c39ca60430.parquet\n",
      "│   └── \u001b[01;34mevent_time_day=2025-12-02\u001b[0m\n",
      "│       └── 00000-0-2cf274a9-e710-4398-a2b8-882ca00b166b.parquet\n",
      "└── \u001b[01;34mmetadata\u001b[0m\n",
      "    ├── 00000-18a99734-edf1-4161-b86e-4b532224fba0.metadata.json\n",
      "    ├── 00001-bf263d9e-8932-4bbc-98bc-f853e8483c9d.metadata.json\n",
      "    ├── 00002-7548f7af-8ee7-48eb-94fc-c30051c7129e.metadata.json\n",
      "    ├── 2cf274a9-e710-4398-a2b8-882ca00b166b-m0.avro\n",
      "    ├── cad83280-0b8f-4a8d-90d7-a7c39ca60430-m0.avro\n",
      "    ├── snap-1977146308163284239-0-2cf274a9-e710-4398-a2b8-882ca00b166b.avro\n",
      "    └── snap-7847261690880971748-0-cad83280-0b8f-4a8d-90d7-a7c39ca60430.avro\n",
      "\n",
      "5 directories, 9 files\n"
     ]
    }
   ],
   "source": [
    "# Verify new partition\n",
    "print(\"\\nTable structure:\")\n",
    "!tree tmp/warehouse/default/events/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363f286",
   "metadata": {},
   "source": [
    "## 4. Time Travel\n",
    "\n",
    "We can query the table at different points in time using snapshot IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9391adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data:\n",
      "           event_time  user_id event_name      event_properties\n",
      "0 2025-12-02 09:15:00        1      login     {\"device\": \"web\"}\n",
      "1 2025-12-02 14:20:00        3   checkout     {\"amount\": 99.99}\n",
      "2 2025-12-01 10:00:00        1      login  {\"device\": \"mobile\"}\n",
      "3 2025-12-01 11:30:00        2  view_item      {\"item_id\": 123}\n",
      "\n",
      "Snapshot History:\n",
      "ID: 7847261690880971748, Timestamp: 2025-12-03 12:34:26.684000\n",
      "ID: 1977146308163284239, Timestamp: 2025-12-03 12:34:31.150000\n",
      "\n",
      "Data at first snapshot (7847261690880971748):\n",
      "           event_time  user_id event_name      event_properties\n",
      "0 2025-12-01 10:00:00        1      login  {\"device\": \"mobile\"}\n",
      "1 2025-12-01 11:30:00        2  view_item      {\"item_id\": 123}\n"
     ]
    }
   ],
   "source": [
    "# Current state\n",
    "print(\"Current data:\")\n",
    "print(table.scan().to_pandas())\n",
    "\n",
    "# History\n",
    "print(\"\\nSnapshot History:\")\n",
    "history = table.history()\n",
    "for snapshot in history:\n",
    "    print(f\"ID: {snapshot.snapshot_id}, Timestamp: {datetime.fromtimestamp(snapshot.timestamp_ms/1000)}\")\n",
    "\n",
    "# Time Travel to first snapshot\n",
    "first_snapshot_id = history[0].snapshot_id\n",
    "print(f\"\\nData at first snapshot ({first_snapshot_id}):\")\n",
    "print(table.scan(snapshot_id=first_snapshot_id).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81db137",
   "metadata": {},
   "source": [
    "## 5. Partition Pruning\n",
    "\n",
    "Querying with a filter on the partition column allows Iceberg to skip unrelated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "499f75d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files selected by the scan:\n",
      "tmp/warehouse/default/events/data/event_time_day=2025-12-02/00000-0-2cf274a9-e710-4398-a2b8-882ca00b166b.parquet\n",
      "\n",
      "Result:\n",
      "           event_time  user_id event_name   event_properties\n",
      "0 2025-12-02 09:15:00        1      login  {\"device\": \"web\"}\n",
      "1 2025-12-02 14:20:00        3   checkout  {\"amount\": 99.99}\n"
     ]
    }
   ],
   "source": [
    "# Query filtering for Day 2\n",
    "scan = table.scan(row_filter=\"event_time >= '2025-12-02T00:00:00'\")\n",
    "\n",
    "print(\"Files selected by the scan:\")\n",
    "for task in scan.plan_files():\n",
    "    print(task.file.file_path)\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "print(scan.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c7f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iceberg-talk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
